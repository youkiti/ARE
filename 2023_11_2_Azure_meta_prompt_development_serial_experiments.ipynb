{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youkiti/ARE/blob/main/2023_11_2_Azure_meta_prompt_development_serial_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sswpT1r-U8bB",
        "outputId": "cccafb2e-8600-4f89-c1b8-bb6a207f21ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqXxBLtDfQhI"
      },
      "source": [
        "#Prepare datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WK9aC1qMcwN",
        "outputId": "f9a82bf1-ded0-4e6f-ae44-380d3aa35fa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q openai cohere tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO_f5DCIMPkd"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2Fh20JtMLJR"
      },
      "source": [
        "#Please apply your own settings to the following cell\n",
        "\n",
        "*   If you dont know the path please click the left folder button.\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB8AAAAkCAYAAABxE+FXAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAEnQAABJ0Ad5mH3gAAAEVSURBVFhH7ZSxaoRAFEWvQRC3t1Ms/AFLSz9iP8LGzg9YOxEre1trQSy1WdDawm+wECxcsNSQ5EEICWFmSZhmTjNzb3MYmPcUz/NOCOKFTiFIuRCkXAhSLgTm9Xq9XuH7PnRdp+aTrutQliXOk29TM8kNw0CSJJR+5n6/YxgGSt9ZlgXrulL6gElumibiOKb0PHVdo6oqSpzybdswzzO1fDiOg8fjgSiKqGGUW5aF2+2Gvu9RFAW1fKRp+v5fwjCkRs65IKT8VxRFodvfIvTlXEtmmiY0TUMtH0EQQFXVL3POJNc0DVmW4XK5UPMc4zgiz3NKjPI3bNuG67qU+DmOA23bYt93ajjk/4GccyFIuRCkXADAK7cPWMgFoxg8AAAAAElFTkSuQmCC)\n",
        "\n",
        "\n",
        "*   Then click the drive folder and seek."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-M-TtSDHAmn"
      },
      "outputs": [],
      "source": [
        "API_KEY = \"YOUR OWN AZURE OPENAI API KEY\"\n",
        "RESOURCE_ENDPOINT = \"YOUR OWN ENDPOINT\"\n",
        "\n",
        "openai.api_type = \"azure\"\n",
        "openai.api_key = API_KEY\n",
        "openai.api_base = RESOURCE_ENDPOINT\n",
        "openai.api_version = '2023-07-01-preview'\n",
        "\n",
        "#PLEASE CHANGE DEPENDING YOUR OWN DEPLOYS\n",
        "deployment_ids = [\"gpt-35-turbo-0613\",\"gpt-35-turbo-16k-0613\",\"gpt4-0613\",\"gpt-4-32k-0613\",\"text-embedding-ada-002\"]\n",
        "\n",
        "folderPath = \"Path to save experiments\"\n",
        "\n",
        "# meta-prompt modification cut-offs\n",
        "sensitivity_cutoff = 0.9\n",
        "specificity_cutoff = 0.5\n",
        "error_proportion_cutoff = 0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bx6Gxizaw8n9"
      },
      "outputs": [],
      "source": [
        "# change directory\n",
        "os.chdir(folderPath)\n",
        "\n",
        "# make a folder to save experiments\n",
        "os.makedirs(\"experiments\", exist_ok=True)\n",
        "\n",
        "# read Excel files with \"reference standards\" for development of meta-prompts\n",
        "df_train100 = pd.read_excel('https://github.com/youkiti/ARE/raw/main/datasets/dataset1.xlsx',engine='openpyxl')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train100.head()"
      ],
      "metadata": {
        "id": "U3JYi_E0BosR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qNbXQHru6Kn"
      },
      "outputs": [],
      "source": [
        "#first prompt\n",
        "prompt = (\"You are a systematic reviewer. Please judge diagnostic test accuracy (DTA) studies from inputted abstracts. \"\n",
        "         \"The definition of a DTA study is an original study that evaluated a test against a clinical reference standard for humans.\"\n",
        "         \"You can accept multivariable diagnostic prediction model studies, but exclude prognostic prediction model studies, that measured predictors and outcomes at different time points.\"\n",
        "         \"Exclude modeling studies, studies that assessed diagnostic training for medical professionals, and case series (e.g., studies without controls, such as following polymerase chain reaction results of specific patients).\"\n",
        "         \"The abstract is as follows:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSS61fAax-k2"
      },
      "outputs": [],
      "source": [
        "# Processing of abstracts\n",
        "def process_abstracts(abstract, prompt, model_name, temperature):\n",
        "    # Construct the prompt\n",
        "    question = prompt + str(abstract)\n",
        "\n",
        "    # Define functions for processing\n",
        "    functions = [\n",
        "        {\n",
        "            \"name\": \"dta_filter\",\n",
        "            \"description\": \"Classify diagnostic test accuracy abstracts\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"judgement\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Determines if the abstract is a diagnostic test accuracy study abstract. Returns 'True' or 'False'.\"\n",
        "                    },\n",
        "                    \"probability\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The probability (from 0 to 1) that the abstract is a diagnostic test accuracy study abstract.\"\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Call Azure API\n",
        "    response = openai.ChatCompletion.create(\n",
        "        deployment_id=model_name,\n",
        "        messages=[{\"role\": \"user\", \"content\": question}],\n",
        "        functions=functions,\n",
        "        function_call=\"auto\",\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    # Parse the result in JSON format\n",
        "    response_json = json.loads(response[\"choices\"][0][\"message\"][\"function_call\"][\"arguments\"])\n",
        "    return response_json\n",
        "\n",
        "# Process DataFrame\n",
        "def process_abstracts_to_dataframe(df_train, prompt, model_name, temperature):\n",
        "    reviews = []\n",
        "    errors = 0\n",
        "    MAX_RETRIES = 3  # Max retries, set to 3 for practical reasons\n",
        "\n",
        "    for index, tiab in tqdm(enumerate(df_train[\"tiab\"])):\n",
        "        response_json = {}  # Initialize response_json\n",
        "        retries = 0\n",
        "        success = False\n",
        "        while retries < MAX_RETRIES and not success:\n",
        "            try:\n",
        "                response_json = process_abstracts(abstract=tiab, prompt=prompt, model_name=model_name, temperature=temperature)\n",
        "                # Confirm that the judgement value is True or False\n",
        "                if response_json['judgement'] not in [\"True\", \"False\"]:\n",
        "                    raise ValueError(\"Judgement value is not True or False\")\n",
        "\n",
        "                # Add index information to response_json\n",
        "                response_json['index'] = index\n",
        "                reviews.append(response_json)\n",
        "                success = True\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error occurred: {e}. Retrying {retries + 1}/{MAX_RETRIES}\")\n",
        "                retries += 1\n",
        "                time.sleep(1)  # Wait for 1 second\n",
        "\n",
        "        if retries == MAX_RETRIES:\n",
        "            print(f\"Failed to process abstract after {MAX_RETRIES} retries.\")\n",
        "            response_json['index'] = index\n",
        "            reviews.append(response_json)\n",
        "            errors += 1\n",
        "\n",
        "    print(f\"Total errors occurred: {errors}\")\n",
        "\n",
        "    # Define column name based on current date and time\n",
        "    now = datetime.now()\n",
        "    date_str = now.strftime('%Y-%m-%d')\n",
        "    time_str = now.strftime('%H-%M')\n",
        "    datetime_str = date_str + \"_\" + time_str\n",
        "    column_name = datetime_str + \"_Azure_\" + \"_\" + model_name + \"_temp\" + str(temperature)\n",
        "\n",
        "    # List for storing index and review information\n",
        "    data = [(review['index'], review) for review in reviews]\n",
        "\n",
        "    # Create DataFrame\n",
        "    df_reviews = pd.DataFrame(data, columns=['index', column_name])\n",
        "\n",
        "    # Set index\n",
        "    df_reviews.set_index('index', inplace=True)\n",
        "\n",
        "    return df_reviews\n",
        "\n",
        "# Function to determine judgement value\n",
        "def judgement_value(cell_value):\n",
        "    if isinstance(cell_value, dict) and 'judgement' in cell_value:\n",
        "        judgement = cell_value['judgement']\n",
        "        if judgement == 'True':\n",
        "            return 1\n",
        "        elif judgement == 'False':\n",
        "            return 0\n",
        "    return np.nan\n",
        "\n",
        "# Function to run experiments\n",
        "def run_experiment(df_train, first_prompt, model_name, folder_path=folderPath, max_iterations=10, temperature=0):\n",
        "    seido = pd.DataFrame()\n",
        "    print(\"model: \" + model_name)\n",
        "    now = datetime.now()\n",
        "    date_str = now.strftime('%Y-%m-%d')\n",
        "    time_str = now.strftime('%H-%M')\n",
        "    datetime_str = date_str + \"_\" + time_str\n",
        "    num = df_train.shape[0]\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        print(f\"iteration: {iteration+1}\")\n",
        "\n",
        "        kekka = process_abstracts_to_dataframe(df_train=df_train, prompt=first_prompt, model_name=model_name, temperature=temperature)\n",
        "        df_train = pd.concat([df_train, kekka], axis=1)\n",
        "\n",
        "        shori_columns = [col for col in df_train.columns[5:] if 'temp' in col and 'shori' not in col]\n",
        "\n",
        "        new_columns = {\"shori_\" + i: df_train[i].apply(judgement_value) for i in shori_columns}\n",
        "        df_train = df_train.assign(**new_columns)\n",
        "\n",
        "        # find \"shori\" column\n",
        "        shori_columns = [col for col in df_train.columns if 'shori' in col]\n",
        "\n",
        "        # right shori\n",
        "        rightmost_shori_column = shori_columns[-1] if shori_columns else None\n",
        "\n",
        "        filled_column = df_train[rightmost_shori_column].fillna(1)\n",
        "        cm = confusion_matrix(df_train[\"judgement\"], filled_column.map({1: 1, 0: 0}))\n",
        "        print(cm)\n",
        "\n",
        "        # calculation of sensitivity and specificity\n",
        "        sensitivity = cm[1, 1] / (cm[1, 1] + cm[1, 0])\n",
        "        specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
        "        nan_count = df_train[rightmost_shori_column].isna().sum()\n",
        "        nanproportion = nan_count / df_train.shape[0]\n",
        "\n",
        "        results_df = pd.DataFrame({\n",
        "            'model': [datetime_str + \"_\" + str(df_train.shape[0]) + \"_\" + model_name + \"temp_\" + str(temperature)],\n",
        "            'prompt': [first_prompt],\n",
        "            'Sensitivity': [sensitivity],\n",
        "            'Specificity': [specificity],\n",
        "            'Proportion of NaN values': [nanproportion],\n",
        "            'TP': cm[1, 1],\n",
        "            'FN': cm[1, 0],\n",
        "            'FP': cm[0, 1],\n",
        "            'TN': cm[0, 0]\n",
        "        })\n",
        "\n",
        "        seido = pd.concat([seido, results_df])\n",
        "\n",
        "        seido.to_excel(folderPath + \"/experiments/\" + datetime_str + \"_\" + str(num) + \"abstracts_serial_experiments\" + model_name + \"_azure_seido.xlsx\", engine='openpyxl', index=False)\n",
        "\n",
        "        # save each iteration\n",
        "        df_train.to_excel(folderPath + \"/experiments/\" + datetime_str + \"_\" + str(num) + \"abstracts_serial_experiments\" + model_name + \"_azure.xlsx\", engine='openpyxl', index=False)\n",
        "\n",
        "        # meta-prompt modification you can change your cut-offs\n",
        "        if sensitivity < sensitivity_cutoff or specificity < specificity_cutoff or nanproportion > error_proportion_cutoff:\n",
        "            print(f\"Iteration {iteration + 1}: Sensitivity was {sensitivity}. Specificity was {specificity}. Error was {nanproportion}. Retrying...\")\n",
        "\n",
        "            # change meta-prompt\n",
        "            response = openai.ChatCompletion.create(\n",
        "                deployment_id=model_name,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"Please become my prompt engineer. Your goal is to help me create the best prompts for systematic review of diagnostic test accuracy. The prompts will be used by you, ChatGPT. Please rewrite inputted metaprompt to achieve sensitivity > 0.9 and specificity > 0.4 and error proportion < 0.1.\"},\n",
        "                    {\"role\": \"user\", \"content\": f\"The sensitivity was {sensitivity}. The specificity was {specificity}. The proportion of errors was {nanproportion}. The prompt was {first_prompt}.\"}\n",
        "                ],\n",
        "                temperature=0.6\n",
        "            )\n",
        "            first_prompt = response.choices[0].message.content\n",
        "            print(first_prompt)\n",
        "            continue\n",
        "\n",
        "        else:\n",
        "            print(f\"Iteration {iteration + 1}: Sensitivity is greater or equal to 0.9. Stopping...\")\n",
        "            break\n",
        "    else:\n",
        "        print(f\"Reached {max_iterations} iterations without reaching the desired sensitivity. Stopping...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF-LWA2zlofI"
      },
      "source": [
        "#execute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YMzfThL0HFjJ"
      },
      "outputs": [],
      "source": [
        "run_experiment(df_train=df_train100, first_prompt=prompt, model_name=deployment_ids[0], max_iterations= 10,temperature = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please check your results in your own google drive folder"
      ],
      "metadata": {
        "id": "dBNCpcKiCO4e"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeoyeDq4ACPSZ12Wc8+5NM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}