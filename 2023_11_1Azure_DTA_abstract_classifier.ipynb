{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youkiti/ARE/blob/main/2023_11_1Azure_DTA_abstract_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#About\n",
        "\n",
        "\n",
        "*   You can use this notebook to classify abstracts as either diagnostic test accuracy studies or not.\n",
        "*   You need\n",
        "1.   to set up the Azure OpenAI API.\n",
        "2.   RIS files of abstracts in the Google drive\n",
        "\n"
      ],
      "metadata": {
        "id": "74c90Y7ww8_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "5osK6va94Ij2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai cohere tiktoken rispy"
      ],
      "metadata": {
        "id": "gtdT8tKQ4E1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aa05402-593b-406c-b69d-9174587a7188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "You should use Google Drive to store your files."
      ],
      "metadata": {
        "id": "7YZ6G6acx-ZL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sswpT1r-U8bB",
        "outputId": "d5e090fb-e1b1-4829-8f45-1ddce0ca83c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import AzureOpenAI\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import math\n",
        "import numpy as np\n",
        "import glob\n",
        "import rispy\n",
        "from pathlib import Path\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "exhj6Med4ZhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Please apply your own settings to the following cell\n",
        "\n",
        "*   If you dont know the path please click the left folder button.\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB8AAAAkCAYAAABxE+FXAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAEnQAABJ0Ad5mH3gAAAEVSURBVFhH7ZSxaoRAFEWvQRC3t1Ms/AFLSz9iP8LGzg9YOxEre1trQSy1WdDawm+wECxcsNSQ5EEICWFmSZhmTjNzb3MYmPcUz/NOCOKFTiFIuRCkXAhSLgTm9Xq9XuH7PnRdp+aTrutQliXOk29TM8kNw0CSJJR+5n6/YxgGSt9ZlgXrulL6gElumibiOKb0PHVdo6oqSpzybdswzzO1fDiOg8fjgSiKqGGUW5aF2+2Gvu9RFAW1fKRp+v5fwjCkRs65IKT8VxRFodvfIvTlXEtmmiY0TUMtH0EQQFXVL3POJNc0DVmW4XK5UPMc4zgiz3NKjPI3bNuG67qU+DmOA23bYt93ajjk/4GccyFIuRCkXADAK7cPWMgFoxg8AAAAAElFTkSuQmCC)\n",
        "\n",
        "\n",
        "*   Then click the drive folder and seek."
      ],
      "metadata": {
        "id": "XXTuD1Wj49Uv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Please enter your API Key\n",
        "API_VERSION = \"2023-07-01-preview\"\n",
        "API_KEY = \"YOUR OWN API KEY\"\n",
        "RESOURCE_ENDPOINT = \"YOUR OWN ENDPOINT\"\n",
        "\n",
        "# gets the API Key from environment variable AZURE_OPENAI_API_KEY\n",
        "client = AzureOpenAI(\n",
        "    api_version=API_VERSION,\n",
        "    api_key = API_KEY,\n",
        "    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n",
        "    azure_endpoint=RESOURCE_ENDPOINT,\n",
        ")\n",
        "\n",
        "#please enter your OWN deploy names\n",
        "deployment_ids = [\"gpt-35-turbo-0613\"]\n",
        "\n",
        "#Please set your folder includes RIS files\n",
        "folderPath = \"YOUR OWN FOLDER\""
      ],
      "metadata": {
        "id": "tv0hCZJp4U47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv6PrEYqefLw"
      },
      "source": [
        "#Processing Bibliographic Information and Creating a Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVFBQrOse3GX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "504ce9d2-26fd-4c31-a474-a467778d6672"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of RIS files:\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "os.chdir(folderPath)\n",
        "#search ris files\n",
        "risfiles = glob.glob(\"*.ris\")\n",
        "print(\"Number of RIS files:\")\n",
        "print(len(risfiles))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3Rd9Xpae6vd"
      },
      "outputs": [],
      "source": [
        "#ris to dataframe\n",
        "#https://github.com/MrTango/rispy\n",
        "\n",
        "def ris_to_df(ris_path):\n",
        "    p = Path(ris_path)\n",
        "\n",
        "    # Open the file and read the data\n",
        "    with p.open() as f:\n",
        "        # Remove lines starting with 'Link to the Ovid Full Text or citation:' and join the remaining lines into a string\n",
        "        data = \"\".join([line for line in f if not line.startswith(\"Link to the Ovid Full Text or citation:\")])\n",
        "\n",
        "    # Parse the RIS data\n",
        "    entries = rispy.loads(data)\n",
        "    print(\"Number of abstracts in the RIS file:\")\n",
        "    print(len(entries))\n",
        "\n",
        "    # Convert to a dataframe\n",
        "    df = pd.json_normalize(entries)\n",
        "    print(\"Column names:\")\n",
        "    print(df.columns)\n",
        "    return df\n",
        "\n",
        "#read\n",
        "df_ris = pd.DataFrame()\n",
        "for i in range(len(risfiles)):\n",
        "    df = ris_to_df(risfiles[i])\n",
        "    df_ris = pd.concat([df_ris,df])\n",
        "\n",
        "print(\"final dataframe:\")\n",
        "print(df_ris.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def select_title(row):\n",
        "    if 'title' in row and pd.notna(row['title']):\n",
        "        return row['title']\n",
        "    elif 'primary_title' in row and pd.notna(row['primary_title']):\n",
        "        return row['primary_title']\n",
        "    elif 'secondary_title' in row and pd.notna(row['secondary_title']):\n",
        "        return row['secondary_title']\n",
        "    elif 'tertiary_title' in row and pd.notna(row['tertiary_title']):\n",
        "        return row['tertiary_title']\n",
        "    return np.nan\n",
        "\n",
        "\n",
        "df = pd.DataFrame()\n",
        "#If this cell doesnt work, check the Column names of df_ris\n",
        "df['selected_title'] = df_ris.apply(select_title, axis=1)\n",
        "df['abstract'] = df_ris['abstract']\n",
        "\n",
        "df[\"tiab\"] = df[\"selected_title\"].fillna('').astype(str) + \" \" + df[\"abstract\"].fillna('').astype(str)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "a9vt69_y1WMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqXxBLtDfQhI"
      },
      "source": [
        "#Azure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qNbXQHru6Kn"
      },
      "outputs": [],
      "source": [
        "#best prompt for DTA SR\n",
        "prompt = (\"Please determine if an abstract is a Diagnostic Test Accuracy (DTA) study based on the following criteria: \"\n",
        "\n",
        "\"1. A DTA study evaluates a test against a clinical reference standard specifically for humans, with very high sensitivity and reasonable specificity.\"\n",
        "\"2. Include multivariable diagnostic prediction model studies.\"\n",
        "\"3. Exclude the following: \"\n",
        "\"   - Prognostic prediction model studies where predictors and outcomes are measured at different time points. \"\n",
        "\"   - Modeling studies. \"\n",
        "\"   - Studies assessing diagnostic training for medical professionals. \"\n",
        "\n",
        "\"Reply with 'True' if the abstract is a DTA study or if there is insufficient information to judge (e.g., when only a title is available). Reply with 'False' if you are certain that the abstract is not a DTA study.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0Gbfc3pux9v"
      },
      "outputs": [],
      "source": [
        "#returns a processed JSON when given an abstract\n",
        "def process_of_abstracts(abstract, prompt, model_name,temperature):\n",
        "    #prompting\n",
        "    question = prompt + str(abstract)\n",
        "\n",
        "    #function calling\n",
        "    functions=[\n",
        "        {\n",
        "            \"name\":\"dta_filter\",\n",
        "            \"description\":\"classify diagnostic test accuracy abstracts\",\n",
        "            \"parameters\":{\n",
        "                \"type\":\"object\",\n",
        "                \"properties\":{\n",
        "                    \"judgement\":{\n",
        "                        \"type\":\"string\",\n",
        "                        \"description\":\"Determining if the abstract is an abstract of diagnostic test accuracy study. The return must be 'True' or 'False'.\"\n",
        "                    },\n",
        "                    \"probability\":{\n",
        "                        \"type\":\"string\",\n",
        "                        \"description\":\"0 to 1 possibility that the abstract is an abstract of diagnostic test accuracy study.\"\n",
        "                    }\n",
        "                    },\n",
        "                #\"required\": [\"judgement\",\"probability\"]\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Azure API\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": question\n",
        "            }],\n",
        "        functions=functions,\n",
        "        function_call = \"auto\",\n",
        "        temperature = temperature\n",
        "    )\n",
        "\n",
        "    #Analyze the results in JSON format.\n",
        "    response_json = response.model_dump_json()\n",
        "    response_json = json.loads(response_json)\n",
        "    response_json = response_json[\"choices\"][0][\"message\"][\"function_call\"][\"arguments\"]\n",
        "    response_json = json.loads(response_json)\n",
        "    return response_json\n",
        "\n",
        "#%%　process a DataFrame\n",
        "def process_abstracts_to_dataframe(df_train,prompt, model_name, temperature):\n",
        "    reviews = []\n",
        "    errors = 0\n",
        "    MAX_RETRIES = 3 #  retry\n",
        "\n",
        "    for index, tiab in tqdm(enumerate(df_train[\"tiab\"])):\n",
        "        res_json = {}  # Initialize res_json\n",
        "        retries = 0\n",
        "        success = False\n",
        "        while retries < MAX_RETRIES and not success:\n",
        "            try:\n",
        "                res_json = process_of_abstracts(abstract=tiab, prompt=prompt, model_name=model_name, temperature=temperature)\n",
        "\n",
        "                # judgementの値がTrueまたはFalseであるか確認\n",
        "                if res_json['judgement'] not in [\"True\", \"False\"]:\n",
        "\n",
        "                    raise ValueError(\"judgement value is not True or False\")\n",
        "\n",
        "                # インデックス情報をres_jsonに追加\n",
        "                res_json['index'] = index\n",
        "                reviews.append(res_json)\n",
        "                success = True\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error occurred: {e}. Retrying {retries + 1}/{MAX_RETRIES}\")\n",
        "                retries += 1\n",
        "                time.sleep(1)  # 1秒待つ\n",
        "\n",
        "        if retries == MAX_RETRIES:\n",
        "            print(f\"Failed to process abstract after {MAX_RETRIES} retries.\")\n",
        "            res_json['index'] = index\n",
        "            reviews.append(res_json)\n",
        "            errors += 1\n",
        "\n",
        "    print(f\"Total errors occurred: {errors}\")\n",
        "\n",
        "    # カラム名を定義\n",
        "    now = datetime.now()\n",
        "    date_str = now.strftime('%Y-%m-%d')  # e.g., \"2023-08-08\"\n",
        "    time_str = now.strftime('%H-%M')  # e.g., \"14:35\"\n",
        "    dtime = date_str +\"_\" + time_str\n",
        "    column_name = dtime+ \"_Azure_\"+\"_\"+ model_name +\"_\"+ \"temp\" + str(temperature)\n",
        "\n",
        "    # インデックスとレビューの情報を格納するリスト\n",
        "    data = [(review['index'], review) for review in reviews]\n",
        "\n",
        "    # DataFrameを作成\n",
        "    df_reviews = pd.DataFrame(data, columns=['index', column_name])\n",
        "\n",
        "    # インデックスを設定\n",
        "    df_reviews.set_index('index', inplace=True)\n",
        "\n",
        "    return df_reviews\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF-LWA2zlofI"
      },
      "source": [
        "#run\n",
        "*   Change the model according to the needs.\n",
        "*   Temperature should ideally be 0.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GNnImq6xLYi"
      },
      "outputs": [],
      "source": [
        "labeled_df = process_abstracts_to_dataframe(df_train = df, prompt=prompt, model_name= deployment_ids[0], temperature = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save"
      ],
      "metadata": {
        "id": "P_UJgDnmBFc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_df['judgement'] = labeled_df.iloc[:,0].apply(lambda x: x['judgement'] == 'True')\n",
        "\n",
        "df_ris = df_ris.join(labeled_df['judgement'])\n",
        "\n",
        "# save\n",
        "df_ris.to_excel('bibliofromRIS_labeled.xlsx', index=False)\n",
        "\n",
        "#select the candidate abstracts\n",
        "df_ris_to_export = df_ris[df_ris['judgement'] == True]\n",
        "df_ris_to_export = df_ris_to_export.drop(columns=['judgement'])\n",
        "\n",
        "print(\"exported abstracs: \"+ str(df_ris_to_export.shape[0]))\n",
        "\n",
        "df_ris_to_export = df_ris_to_export.fillna('')\n",
        "# to list\n",
        "export_ris = df_ris_to_export.to_dict('records')\n",
        "# export\n",
        "with open('selected_abstracts.ris', 'w') as bibliography_file:\n",
        "    rispy.dump(export_ris, bibliography_file)"
      ],
      "metadata": {
        "id": "ynLG5HZo-Y0s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yv6PrEYqefLw"
      ],
      "authorship_tag": "ABX9TyNUs3pOeltCwpo7P5p8T4eq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}